# -*- coding: utf-8 -*-
"""OMOLab5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bfbkjaiM3YzoUbh4VmcyCP0FVKxSYSGl
"""

import numpy as np
import matplotlib.pyplot as plt
a, b, c, d = 0.2, 0.2, 0.06, 0.2
n_inputs = 8
n_hidden = 3

def generate_data(n_samples=1000):
    x = np.linspace(0, 10, n_samples)
    y = a * np.cos(d * x) + c * np.sin(d * x)
    return x, y

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return x * (1 - x)

def linear(x):
    return x

def linear_derivative(x):
    return 1

def initialize_weights(n_inputs, n_hidden):
    np.random.seed(42)
    W1 = np.random.randn(n_inputs, n_hidden) * 0.1
    b1 = np.zeros((1, n_hidden))
    W2 = np.random.randn(n_hidden, 1) * 0.1
    b2 = np.zeros((1, 1))
    return W1, b1, W2, b2

def forward_propagation(X, W1, b1, W2, b2):
    hidden_input = np.dot(X, W1) + b1
    hidden_output = sigmoid(hidden_input)
    output = np.dot(hidden_output, W2) + b2
    return hidden_output, output

def backward_propagation(X, y, hidden_output, output, W2, learning_rate):
    m = X.shape[0]

    error_output = output - y.reshape(-1, 1)
    d_output = error_output * linear_derivative(output)

    error_hidden = d_output.dot(W2.T)
    d_hidden = error_hidden * sigmoid_derivative(hidden_output)

    dW2 = hidden_output.T.dot(d_output) / m
    db2 = np.sum(d_output, axis=0, keepdims=True) / m
    dW1 = X.T.dot(d_hidden) / m
    db1 = np.sum(d_hidden, axis=0, keepdims=True) / m

    return dW1, db1, dW2, db2, error_output

def update_weights(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):
    W1 -= learning_rate * dW1
    b1 -= learning_rate * db1
    W2 -= learning_rate * dW2
    b2 -= learning_rate * db2
    return W1, b1, W2, b2

def prepare_sequences(data, n_inputs):
    X, y = [], []
    for i in range(len(data) - n_inputs):
        X.append(data[i:i+n_inputs])
        y.append(data[i+n_inputs])
    return np.array(X), np.array(y)

def train_neural_network(X_train, y_train, n_inputs, n_hidden, learning_rate=0.01, epochs=1000):
    W1, b1, W2, b2 = initialize_weights(n_inputs, n_hidden)

    errors = []

    for epoch in range(epochs):
        hidden_output, output = forward_propagation(X_train, W1, b1, W2, b2)

        dW1, db1, dW2, db2, error_output = backward_propagation(
            X_train, y_train, hidden_output, output, W2, learning_rate)

        W1, b1, W2, b2 = update_weights(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)

        mse = np.mean(error_output ** 2)
        errors.append(mse)

        if epoch % 100 == 0:
            print(f"Эпоха {epoch}, MSE: {mse:.6f}")

    return W1, b1, W2, b2, errors

def predict(X, W1, b1, W2, b2):
    hidden_output, output = forward_propagation(X, W1, b1, W2, b2)
    return output.flatten()

x, y = generate_data(500)

X, Y = prepare_sequences(y, n_inputs)

split_idx = int(0.8 * len(X))
X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = Y[:split_idx], Y[split_idx:]

print(f"Размер обучающей выборки: {X_train.shape}")
print(f"Размер тестовой выборки: {X_test.shape}")

W1, b1, W2, b2, errors = train_neural_network(X_train, y_train, n_inputs, n_hidden,
                                              learning_rate=0.1, epochs=2000)

y_train_pred = predict(X_train, W1, b1, W2, b2)

y_test_pred = predict(X_test, W1, b1, W2, b2)

train_errors = np.abs(y_train - y_train_pred)
print(f"Средняя ошибка на обучении: {np.mean(train_errors):.6f}")

test_errors = np.abs(y_test - y_test_pred)
print(f"Средняя ошибка на тесте: {np.mean(test_errors):.6f}")

plt.figure(figsize=(15, 10))

plt.subplot(2, 2, 1)
plt.plot(x[:len(y_train)], y_train, 'b-', label='Истинные значения', alpha=0.7)
plt.plot(x[:len(y_train)], y_train_pred, 'r--', label='Прогноз', alpha=0.7)
plt.title('Прогнозируемая функция на участке обучения')
plt.xlabel('Время')
plt.ylabel('y')
plt.legend()
plt.grid(True)

plt.subplot(2, 2, 2)
plt.plot(errors)
plt.title('Изменение ошибки в процессе обучения')
plt.xlabel('Итерация')
plt.ylabel('MSE')
plt.grid(True)
plt.yscale('log')


plt.tight_layout()
plt.show()

print("\nТаблица результатов обучения (первые 10 примеров):")
print("Эталонное\tПолученное\tОтклонение")
for i in range(min(10, len(y_train))):
    print(f"{y_train[i]:.6f}\t{y_train_pred[i]:.6f}\t{abs(y_train[i] - y_train_pred[i]):.6f}")

print("\nТаблица результатов прогнозирования (первые 10 примеров):")
print("Эталонное\tПолученное\tОтклонение")
for i in range(min(10, len(y_test))):
    print(f"{y_test[i]:.6f}\t{y_test_pred[i]:.6f}\t{abs(y_test[i] - y_test_pred[i]):.6f}")